server.port=8891

langchain4j.open-ai.chat-model.api-key=sk-wrPsPqcp9Jh2X4ZYdFRw2c7bK9Ygt8iLSkw9pVl98yikGmCk
#langchain4j.open-ai.chat-model.model-name=gpt-4o-mini
#langchain4j.open-ai.chat-model.base-url=http://langchain4j.dev/demo/openai/v1

langchain4j.open-ai.chat-model.model-name=deepseek-chat
langchain4j.open-ai.chat-model.base-url=https://aiproxy.hzh.sealos.run/v1

langchain4j.open-ai.chat-model.log-requests=true
langchain4j.open-ai.chat-model.log-responses=true

# ollama ????
langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=deepseek-r1:1.5b
langchain4j.ollama.chat-model.temperature=0.8
langchain4j.ollama.chat-model.timeout=PT60S

langchain4j.ollama.chat-model.log-requests=true
langchain4j.ollama.chat-model.log-responses=true

# ???????
langchain4j.community.dashscope.chat-model.api-key=sk-b87fccf9a9aa4df49f62b63945a74c3f
langchain4j.community.dashscope.chat-model.model-name=qwen-max


#????debug??
logging.level.root=debug